import { MODELS } from './mockData';

// export type Role = keyof typeof Roles
// type Permission = (typeof Roles)[Role][number];

// const Roles = {
//   admin : [
//     "view:comments",
//     "create:comments",
//     "update:comments",
//     "delete:comments",
//   ],
//   moderator : [
//     "view:comments",
//     "create:comments",
//     "update:comments",
//   ],
//   user: [
//     "view:own:comments",
//     "create:comments",
//     "update:own:comments",
//   ]
// } as const

// export function hasPermission(user: {id:string; role:Role}, permission: Permission) {
//     return (Roles[user.role] as readonly Permission[]).includes(permission)
// }

// Simulates API delay
const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

// Mock API responses based on model
const getMockResponse = (model: string, prompt: string): string => {
  const responses: Record<string, string[]> = {
    'gpt-4': [
      "I understand your request. Let me provide a detailed analysis...",
      "Based on the code you've shared, here are my observations...",
      "That's an interesting question. Here's what I think...",
    ],
    'gpt-3.5': [
      "Sure! Here's a quick response to your question...",
      "I can help with that. Let me explain...",
      "Great question! Here's my take...",
    ],
    'claude-opus': [
      "I've carefully analyzed your request. Here's a comprehensive breakdown...",
      "Let me provide a thorough examination of this topic...",
      "I appreciate the detailed question. Allow me to elaborate...",
    ],
    'gemini-pro': [
      "I can process this multimodally. Here's my analysis...",
      "Based on multiple perspectives, here's what I found...",
      "Let me integrate various sources to answer this...",
    ],
  };

  const modelResponses = responses[model] || responses['gpt-4'];
  const randomResponse = modelResponses[Math.floor(Math.random() * modelResponses.length)];

  // Add some context from the prompt
  const promptPreview = prompt.slice(0, 50);
  return `${randomResponse}\n\nRegarding "${promptPreview}...":\n\nThis is a simulated response. In a real implementation, this would be generated by the ${MODELS.find(m => m.id === model)?.name} model based on your specific prompt and parameters.`;
};

export interface ChatCompletionRequest {
    model : string;
    prompt : string;
    temperature: number;
    maxTokens : number;
    topP : number;
    frequencyPenalty : number;
}

export interface ChatCompletionResponse {
    id : string;
    model : string;
    content: string;
    usage: {
        promptTokens : number;
        completionTokens : number;
        totalTokens : number;
    }
}

// Mock Chat completion API

export async function createChatCompletion ( request :ChatCompletionRequest) : Promise<ChatCompletionResponse> {

    // Simulate API delay based on model speed
    const model = MODELS.find(m => m.id === request.model);
    const delayMs = model?.speed === 'fast' ? 500 : model?.speed === 'medium' ? 1000 : 1500;

    await delay(delayMs);

    // Generate mock response
    const content = getMockResponse(request.model, request.prompt);

    // Calculate mock token usage
    const promptTokens = Math.floor(request.prompt.length / 4);
    const completionTokens = Math.floor(content.length / 4);

    return {
        id: `resp-${Date.now()}`,
        model: request.model,
        content,
        usage : {
            promptTokens,
            completionTokens,
            totalTokens: promptTokens + completionTokens,
        }
    }
}

export async function getAvailableModels() {
  await delay(100);
  return MODELS;
}

export { TEMPLATES } from './mockData';